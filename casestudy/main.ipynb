{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step1: tien xu ly file pdf txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\chat\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang xử lý file PDF: D:\\DATN\\QA_System\\casestudy\\pdf\\20230710 1. QĐ Học bổng KKHT 2023.pdf\n",
      "Đã xử lý và lưu file thành công vào D:\\DATN\\QA_System\\casestudy\\pdf\\20230710 1. QĐ Học bổng KKHT 2023.txt\n",
      "Đang xử lý file PDF: D:\\DATN\\QA_System\\casestudy\\pdf\\20230710 2. QĐ Học bổng Trần Đại Nghĩa 2023.pdf\n",
      "Đã xử lý và lưu file thành công vào D:\\DATN\\QA_System\\casestudy\\pdf\\20230710 2. QĐ Học bổng Trần Đại Nghĩa 2023.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "def format_text(text: str) -> str:\n",
    "    \"\"\"Định dạng lại văn bản theo quy tắc mong muốn.\"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    formatted_lines = []\n",
    "    current_paragraph = []\n",
    "    previous_line_numbered = False\n",
    "\n",
    "    for line in lines:\n",
    "        stripped_line = line.strip()\n",
    "\n",
    "        # Loại bỏ các dòng trống liên tiếp\n",
    "        if not stripped_line:\n",
    "            if current_paragraph:  # Nếu có đoạn văn, kết thúc đoạn văn hiện tại\n",
    "                formatted_lines.append(' '.join(current_paragraph))\n",
    "                current_paragraph = []\n",
    "            continue  # Bỏ qua dòng trống\n",
    "\n",
    "        # Xử lý các dòng bắt đầu bằng số hoặc ký tự a), b), c)...\n",
    "        if re.match(r'^(\\d+\\.|\\w\\))', stripped_line):\n",
    "            if current_paragraph:\n",
    "                formatted_lines.append(' '.join(current_paragraph))\n",
    "                current_paragraph = []\n",
    "            current_paragraph.append(stripped_line)\n",
    "            previous_line_numbered = True\n",
    "        # Xử lý các dòng bắt đầu bằng \"Điều\"\n",
    "        elif stripped_line.startswith('Điều'):\n",
    "            if current_paragraph:\n",
    "                formatted_lines.append(' '.join(current_paragraph))\n",
    "                current_paragraph = []\n",
    "            formatted_lines.append(stripped_line)\n",
    "            previous_line_numbered = False\n",
    "        # Xử lý các dòng còn lại\n",
    "        elif stripped_line:\n",
    "            if previous_line_numbered and not stripped_line[0].isupper():\n",
    "                if current_paragraph:  # Kiểm tra nếu current_paragraph có phần tử\n",
    "                    current_paragraph[-1] += ' ' + stripped_line\n",
    "                else:\n",
    "                    current_paragraph.append(stripped_line)  # Nếu không có phần tử, thêm vào\n",
    "            else:\n",
    "                current_paragraph.append(stripped_line)\n",
    "            previous_line_numbered = False\n",
    "\n",
    "    # Nếu còn đoạn văn chưa được thêm vào\n",
    "    if current_paragraph:\n",
    "        formatted_lines.append(' '.join(current_paragraph))\n",
    "\n",
    "    # Kết hợp các đoạn văn thành văn bản\n",
    "    result_text = '\\n'.join(formatted_lines)\n",
    "\n",
    "    # Loại bỏ các dòng trống giữa các đoạn văn\n",
    "    result_text = re.sub(r'\\n+', '\\n', result_text)\n",
    "\n",
    "    return result_text\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Trích xuất văn bản từ file PDF.\"\"\"\n",
    "    try:\n",
    "        elements = partition_pdf(pdf_path, strategy=\"fast\")\n",
    "        return '\\n'.join(e.text if hasattr(e, \"text\") else e for e in elements)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Lỗi khi trích xuất text từ PDF: {e}\")\n",
    "\n",
    "def process_pdf(pdf_path: str, output_folder: str) -> None:\n",
    "    \"\"\"Xử lý và lưu nội dung tệp PDF đã định dạng.\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(pdf_path):\n",
    "            raise FileNotFoundError(f\"Không tìm thấy file PDF: {pdf_path}\")\n",
    "\n",
    "        print(f\"Đang xử lý file PDF: {pdf_path}\")\n",
    "        extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "        formatted_text = format_text(extracted_text)\n",
    "        \n",
    "        # Lấy tên file PDF (không bao gồm phần mở rộng)\n",
    "        pdf_filename = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "        output_path = os.path.join(output_folder, f\"{pdf_filename}.txt\")\n",
    "\n",
    "        with open(output_path, 'w', encoding='utf-8') as file:\n",
    "            file.write(formatted_text)\n",
    "        \n",
    "        print(f\"Đã xử lý và lưu file thành công vào {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Đã xảy ra lỗi trong quá trình xử lý PDF: {e}\")\n",
    "\n",
    "def main():\n",
    "    pdf_folder = r\"D:\\DATN\\QA_System\\casestudy\\pdf\"\n",
    "    output_folder = r\"D:\\DATN\\QA_System\\casestudy\\pdf\"\n",
    "    for filename in os.listdir(pdf_folder):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(pdf_folder, filename)\n",
    "            process_pdf(pdf_path, output_folder)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step2. load and chunking + raptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\chat\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Processing files:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã load dữ liệu từ file: D:\\DATN\\QA_System\\casestudy\\pdf\\20230710 1. QĐ Học bổng KKHT 2023.txt\n",
      "Initializing RaptorPipeline...\n",
      "Initializing Vietnamese embedding model: dangvantuan/vietnamese-embedding\n",
      "Embedding model loaded.\n",
      "Summarization model loaded.\n",
      "Tokenizer loaded.\n",
      "Starting recursive_embed_cluster_summarize for level 1...\n",
      "Processing base level (level 0)...\n",
      "Starting embed_cluster_summarize for level 0...\n",
      "Getting clusters...\n",
      "Embedding texts...\n",
      "Texts embedded. Number of embeddings: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\chat\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters obtained. Number of clusters: 5\n",
      "Generating summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing clusters (level 0): 100%|██████████| 5/5 [00:31<00:00,  6.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries generated.\n",
      "Finished embed_cluster_summarize for level 0.\n",
      "Base level processing completed.\n",
      "Processing level 1...\n",
      "Starting embed_cluster_summarize for level 1...\n",
      "Getting clusters...\n",
      "Embedding texts...\n",
      "Texts embedded. Number of embeddings: 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\chat\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters obtained. Number of clusters: 5\n",
      "Generating summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing clusters (level 1): 100%|██████████| 5/5 [00:30<00:00,  6.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries generated.\n",
      "Finished embed_cluster_summarize for level 1.\n",
      "Level 1 processing completed.\n",
      "Recursing to level 2...\n",
      "Starting recursive_embed_cluster_summarize for level 2...\n",
      "Processing level 2...\n",
      "Starting embed_cluster_summarize for level 2...\n",
      "Getting clusters...\n",
      "Embedding texts...\n",
      "Texts embedded. Number of embeddings: 5\n",
      "Clusters obtained. Number of clusters: 1\n",
      "Generating summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing clusters (level 2): 100%|██████████| 1/1 [00:06<00:00,  6.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries generated.\n",
      "Finished embed_cluster_summarize for level 2.\n",
      "Level 2 processing completed.\n",
      "Finished recursive_embed_cluster_summarize for level 2.\n",
      "Recursion to level 2 completed.\n",
      "Finished recursive_embed_cluster_summarize for level 1.\n",
      "Building final dataframe...\n",
      "Processing chunks (level 0)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 24/24 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing summaries (levels > 0)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing summary levels: 100%|██████████| 2/2 [00:00<00:00, 121.94it/s]\n",
      "Processing files:  50%|█████     | 1/2 [01:29<01:29, 89.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating metadata...\n",
      "Aggregating metadata...\n",
      "Aggregating metadata...\n",
      "Aggregating metadata...\n",
      "Aggregating metadata...\n",
      "Aggregating metadata...\n",
      "Final dataframe built.\n",
      "Đã lưu kết quả RAPTOR cho file D:\\DATN\\QA_System\\casestudy\\pdf\\20230710 1. QĐ Học bổng KKHT 2023.txt vào D:\\DATN\\QA_System\\casestudy\\pdf\\20230710 1. QĐ Học bổng KKHT 2023_results.csv\n",
      "Đã load dữ liệu từ file: D:\\DATN\\QA_System\\casestudy\\pdf\\20230710 2. QĐ Học bổng Trần Đại Nghĩa 2023.txt\n",
      "Initializing RaptorPipeline...\n",
      "Embedding model loaded.\n",
      "Summarization model loaded.\n",
      "Tokenizer loaded.\n",
      "Starting recursive_embed_cluster_summarize for level 1...\n",
      "Processing base level (level 0)...\n",
      "Starting embed_cluster_summarize for level 0...\n",
      "Getting clusters...\n",
      "Embedding texts...\n",
      "Texts embedded. Number of embeddings: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\chat\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters obtained. Number of clusters: 8\n",
      "Generating summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing clusters (level 0): 100%|██████████| 8/8 [00:49<00:00,  6.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries generated.\n",
      "Finished embed_cluster_summarize for level 0.\n",
      "Base level processing completed.\n",
      "Processing level 1...\n",
      "Starting embed_cluster_summarize for level 1...\n",
      "Getting clusters...\n",
      "Embedding texts...\n",
      "Texts embedded. Number of embeddings: 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\envs\\chat\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters obtained. Number of clusters: 8\n",
      "Generating summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing clusters (level 1): 100%|██████████| 8/8 [00:48<00:00,  6.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries generated.\n",
      "Finished embed_cluster_summarize for level 1.\n",
      "Level 1 processing completed.\n",
      "Recursing to level 2...\n",
      "Starting recursive_embed_cluster_summarize for level 2...\n",
      "Processing level 2...\n",
      "Starting embed_cluster_summarize for level 2...\n",
      "Getting clusters...\n",
      "Embedding texts...\n",
      "Texts embedded. Number of embeddings: 8\n",
      "Clusters obtained. Number of clusters: 1\n",
      "Generating summaries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing clusters (level 2): 100%|██████████| 1/1 [00:07<00:00,  7.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries generated.\n",
      "Finished embed_cluster_summarize for level 2.\n",
      "Level 2 processing completed.\n",
      "Finished recursive_embed_cluster_summarize for level 2.\n",
      "Recursion to level 2 completed.\n",
      "Finished recursive_embed_cluster_summarize for level 1.\n",
      "Building final dataframe...\n",
      "Processing chunks (level 0)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 37/37 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing summaries (levels > 0)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing summary levels: 100%|██████████| 2/2 [00:00<00:00, 81.98it/s]\n",
      "Processing files: 100%|██████████| 2/2 [03:26<00:00, 103.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating metadata...\n",
      "Aggregating metadata...\n",
      "Aggregating metadata...\n",
      "Aggregating metadata...\n",
      "Aggregating metadata...\n",
      "Aggregating metadata...\n",
      "Aggregating metadata...\n",
      "Aggregating metadata...\n",
      "Aggregating metadata...\n",
      "Final dataframe built.\n",
      "Đã lưu kết quả RAPTOR cho file D:\\DATN\\QA_System\\casestudy\\pdf\\20230710 2. QĐ Học bổng Trần Đại Nghĩa 2023.txt vào D:\\DATN\\QA_System\\casestudy\\pdf\\20230710 2. QĐ Học bổng Trần Đại Nghĩa 2023_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from data_ingestion import TXTProcessor  # step1_loaddata\n",
    "from chunking import text_splitter  # step2_chunking\n",
    "from raptor import RaptorPipeline  # step3_RAPTOR\n",
    "from utils import convert_df_to_documents\n",
    "from model_config import VietnameseEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import json\n",
    "# Các bước pipeline\n",
    "def step1_load_data(txt_file: str) -> List:\n",
    "    \"\"\"\n",
    "    Load dữ liệu từ một file .txt cụ thể.\n",
    "    \"\"\"\n",
    "    processor = TXTProcessor()\n",
    "    documents = processor.setup_txt(txt_file)\n",
    "    print(f\"Đã load dữ liệu từ file: {txt_file}\")\n",
    "    return documents\n",
    "\n",
    "def step2_chunking(documents):\n",
    "    \"\"\"\n",
    "    Chunk tài liệu thành các phần nhỏ hơn.\n",
    "    \"\"\"\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk.metadata[\"id\"] = str(i)\n",
    "    chunks_metadata = [chunk.metadata for chunk in chunks]\n",
    "    chunks_content = [chunk.page_content for chunk in chunks]\n",
    "    return chunks_metadata, chunks_content\n",
    "\n",
    "def step3_RAPTOR(chunks_metadata, chunks_content):\n",
    "    \"\"\"\n",
    "    Thực hiện RAPTOR pipeline trên chunks.\n",
    "    \"\"\"\n",
    "    raptor = RaptorPipeline()\n",
    "    results = raptor.recursive_embed_cluster_summarize(chunks_content, chunks_metadata, level=1, n_levels=3)\n",
    "    final_df = raptor.build_final_dataframe(results)\n",
    "    return final_df\n",
    "\n",
    "def process_directory(directory: str):\n",
    "    \"\"\"\n",
    "    Đọc và xử lý toàn bộ các file trong thư mục với RAPTOR.\n",
    "    \"\"\"\n",
    "    processor = TXTProcessor(directory=directory)\n",
    "    txt_files = processor.get_txt_files()\n",
    "    \n",
    "    for txt_file in tqdm(txt_files, desc=\"Processing files\"):\n",
    "        # Bước 1: Load dữ liệu\n",
    "        documents = step1_load_data(txt_file)\n",
    "        \n",
    "        # Bước 2: Chunking\n",
    "        chunks_metadata, chunks_content = step2_chunking(documents)\n",
    "        \n",
    "        # Bước 3: RAPTOR\n",
    "        final_df = step3_RAPTOR(chunks_metadata, chunks_content)\n",
    "        \n",
    "        # Lưu kết quả thành file CSV\n",
    "        output_csv = f\"{os.path.splitext(txt_file)[0]}_results.csv\"\n",
    "        final_df[\"metadata\"] = final_df[\"metadata\"].apply(lambda x: json.dumps(x, ensure_ascii=False))\n",
    "        final_df.to_csv(output_csv, index=False, encoding='utf-8-sig')\n",
    "        print(f\"Đã lưu kết quả RAPTOR cho file {txt_file} vào {output_csv}\")\n",
    "        # # Lưu kết quả thành file Pickle\n",
    "        # output_pkl = f\"{os.path.splitext(txt_file)[0]}_results.pkl\"\n",
    "        # final_df.to_pickle(output_pkl)  # Lưu DataFrame dưới dạng Pickle\n",
    "        # print(f\"Đã lưu kết quả RAPTOR cho file {txt_file} vào {output_pkl}\")\n",
    "# Thực thi pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    directory_path = r\"D:\\DATN\\QA_System\\casestudy\\pdf\"  # Thay bằng đường dẫn thực tế\n",
    "    process_directory(directory_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ghep thanh 1 file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tất cả các tệp CSV đã được ghép vào D:\\DATN\\QA_System\\casestudy\\pdf\\merged_output.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def merge_csv_files(input_directory: str, output_file: str):\n",
    "    # Tạo danh sách tất cả các tệp CSV trong thư mục\n",
    "    csv_files = glob.glob(os.path.join(input_directory, \"*.csv\"))\n",
    "    \n",
    "    # Đọc và ghép tất cả các tệp CSV lại thành một DataFrame\n",
    "    df_list = []  # Danh sách để lưu các DataFrame\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)  # Đọc tệp CSV\n",
    "        df_list.append(df)  # Thêm DataFrame vào danh sách\n",
    "    \n",
    "    # Ghép tất cả các DataFrame lại thành một\n",
    "    merged_df = pd.concat(df_list, ignore_index=True)  # ignore_index để không giữ chỉ số ban đầu\n",
    "    print(f\"Tất cả các tệp CSV đã được ghép vào {output_file}\")\n",
    "    # Lưu DataFrame đã ghép vào tệp CSV mới\n",
    "    df = merged_df\n",
    "    df.head()\n",
    "    # Lưu lại DataFrame vào excel mới\n",
    "    df.to_csv(output_file, index=False ,encoding='utf-8') \n",
    "    df.to_excel(execfile, index=False, engine=\"openpyxl\")\n",
    "    \n",
    "\n",
    "# Sử dụng hàm\n",
    "input_directory = r\"D:\\DATN\\QA_System\\casestudy\\pdf\"  # Thư mục chứa các tệp CSV\n",
    "output_file = r\"D:\\DATN\\QA_System\\casestudy\\pdf\\merged_output.csv\"  # Tên tệp CSV đầu ra\n",
    "execfile = r\"D:\\DATN\\QA_System\\casestudy\\pdf\\df.xlsx\"\n",
    "merge_csv_files(input_directory, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chuyen sang json de luu vao db cho nhanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "execfile = r\"D:\\DATN\\QA_System\\casestudy\\pdf\\finaldf.xlsx\"\n",
    "df = pd.read_excel(execfile)\n",
    "import json\n",
    "\n",
    "# Chuyển đổi cột metadata từ chuỗi JSON thành dictionary (nếu cần)\n",
    "if isinstance(df[\"metadata\"].iloc[0], str):  # Kiểm tra kiểu dữ liệu của phần tử đầu tiên trong cột metadata\n",
    "    df[\"metadata\"] = df[\"metadata\"].apply(lambda x: json.loads(x))\n",
    "\n",
    "# Chuyển đổi DataFrame thành danh sách các dictionary\n",
    "json_data = df.to_dict(orient=\"records\")\n",
    "\n",
    "# Lưu danh sách các dictionary vào file JSON\n",
    "output_json_file = r\"D:\\DATN\\QA_System\\casestudy\\pdf\\finaldf.json\"  # Đường dẫn đến file JSON đầu ra\n",
    "with open(output_json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(json_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Đã lưu dữ liệu vào file JSON: {output_json_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loc theo level neu muon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lọc và lưu dữ liệu vào file D:\\DATN\\QA_System\\casestudy\\pdf\\data0.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def filter_json_by_level(input_file, output_file, target_level=0):\n",
    "    \"\"\"\n",
    "    Lọc các phần tử trong file JSON dựa trên giá trị của 'level'.\n",
    "\n",
    "    Args:\n",
    "        input_file: Đường dẫn đến file JSON đầu vào.\n",
    "        output_file: Đường dẫn đến file JSON đầu ra (sẽ ghi đè lên file đầu vào nếu giống nhau).\n",
    "        target_level: Giá trị 'level' cần giữ lại.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file {input_file}\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Lỗi: File {input_file} không đúng định dạng JSON.\")\n",
    "        return\n",
    "\n",
    "    filtered_data = [\n",
    "        item for item in data\n",
    "        if \"level\" in item and item[\"level\"] == target_level\n",
    "    ]\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(filtered_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Đã lọc và lưu dữ liệu vào file {output_file}\")\n",
    "\n",
    "# Sử dụng hàm:\n",
    "input_json_file = r\"D:\\DATN\\QA_System\\casestudy\\pdf\\data.json\"\n",
    "output_json_file = r\"D:\\DATN\\QA_System\\casestudy\\pdf\\data0.json\"\n",
    "filter_json_by_level(input_json_file, output_json_file, target_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document 0\n",
      "Processing document 1\n",
      "Total documents truncated: 0\n",
      "Indexing completed. Time taken: 0.47 seconds\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "import json\n",
    "from time import time\n",
    "es = Elasticsearch(  #new 12/01/25\n",
    "    \"https://my-elasticsearch-project-cb212d.es.ap-southeast-1.aws.elastic.cloud:443\",\n",
    "    api_key=\"eXlld1daUUI2VXkxblVYQ2NZSDU6UXhkWjRxM29RNnU4RUZMY0xYVlk0UQ==\"\n",
    ")\n",
    "\n",
    "# es = Elasticsearch(\n",
    "#     \"https://my-elasticsearch-project-fc9fd1.es.ap-southeast-1.aws.elastic.cloud:443\",\n",
    "#     api_key=\"aXY4NkpKUUJaNVNfUEdnZHdVZ186MExmVk1iclZRWWlrS1hpeDRhOWRGUQ==\"\n",
    "# )\n",
    "\n",
    "from model_config import load_embedding_model_VN2, load_tokenizer2\n",
    "embeddings = load_embedding_model_VN2()\n",
    "tokenizer = load_tokenizer2()\n",
    "json_file_path = r\"D:\\DATN\\QA_System\\casestudy\\pdf\\qa_gen.json\"\n",
    "index_name = \"faq_data\"\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"answer\": {\"type\": \"text\"},\n",
    "            \"category\": {\"type\": \"keyword\"},\n",
    "            \"vector\": {\"type\": \"dense_vector\", \"dims\": 768, \"similarity\": \"cosine\", \"index\": True}  # Số chiều của vector embedding\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "if es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "es.indices.create(index=index_name, body=mapping)\n",
    "\n",
    "def generate_actions(documents):\n",
    "    truncated = 0\n",
    "    for i, record in enumerate(documents):\n",
    "        try:\n",
    "            # Get tokens\n",
    "            tokens = tokenizer.encode(record[\"question\"])\n",
    "            \n",
    "            # Truncate if needed\n",
    "            if len(tokens) > 254:\n",
    "                token = tokens[:236]  # Chỉ lấy 254 token đầu tiên\n",
    "                record[\"question\"] = tokenizer.decode(token)  # Chuyển lại thành text\n",
    "                truncated += 1\n",
    "                print(f\"Truncated document {i} from {len(tokens)} tokens to 236 tokens\")\n",
    "                \n",
    "            embedding = embeddings.embed_query(record[\"question\"])\n",
    "            print(f\"Processing document {i}\")\n",
    "            yield {\n",
    "                    \"_op_type\": \"index\",  # Chỉ mục bản ghi\n",
    "                    \"_index\": index_name,\n",
    "                    \"_source\": {\n",
    "                        \"question\": record[\"question\"],\n",
    "                        \"answer\": record[\"answer\"],\n",
    "                        \"category\": record[\"category\"],\n",
    "                        \"vector\": embedding\n",
    "                    }\n",
    "    \n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing document {i}: {str(e)}\")\n",
    "            raise e\n",
    "    print(f\"Total documents truncated: {truncated}\")\n",
    "\n",
    "\n",
    "with open(json_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "start_time = time()\n",
    "helpers.bulk(es, generate_actions(json_data))\n",
    "print(f\"Indexing completed. Time taken: {time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing document 0\n",
      "Processing document 1\n",
      "Processing document 2\n",
      "Processing document 3\n",
      "Processing document 4\n",
      "Processing document 5\n",
      "Processing document 6\n",
      "Processing document 7\n",
      "Processing document 8\n",
      "Processing document 9\n",
      "Processing document 10\n",
      "Processing document 11\n",
      "Processing document 12\n",
      "Processing document 13\n",
      "Processing document 14\n",
      "Processing document 15\n",
      "Processing document 16\n",
      "Processing document 17\n",
      "Processing document 18\n",
      "Processing document 19\n",
      "Processing document 20\n",
      "Processing document 21\n",
      "Processing document 22\n",
      "Processing document 23\n",
      "Processing document 24\n",
      "Processing document 25\n",
      "Processing document 26\n",
      "Processing document 27\n",
      "Processing document 28\n",
      "Processing document 29\n",
      "Processing document 30\n",
      "Processing document 31\n",
      "Processing document 32\n",
      "Processing document 33\n",
      "Processing document 34\n",
      "Processing document 35\n",
      "Processing document 36\n",
      "Processing document 37\n",
      "Processing document 38\n",
      "Processing document 39\n",
      "Processing document 40\n",
      "Processing document 41\n",
      "Processing document 42\n",
      "Processing document 43\n",
      "Processing document 44\n",
      "Processing document 45\n",
      "Processing document 46\n",
      "Processing document 47\n",
      "Processing document 48\n",
      "Processing document 49\n",
      "Processing document 50\n",
      "Processing document 51\n",
      "Processing document 52\n",
      "Processing document 53\n",
      "Processing document 54\n",
      "Processing document 55\n",
      "Processing document 56\n",
      "Processing document 57\n",
      "Processing document 58\n",
      "Processing document 59\n",
      "Processing document 60\n",
      "Total documents truncated: 0\n",
      "Indexing completed. Time taken: 10.41 seconds\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "import json\n",
    "from time import time\n",
    "es = Elasticsearch(\n",
    "    \"https://my-elasticsearch-project-cb212d.es.ap-southeast-1.aws.elastic.cloud:443\",\n",
    "    api_key=\"eXlld1daUUI2VXkxblVYQ2NZSDU6UXhkWjRxM29RNnU4RUZMY0xYVlk0UQ==\"\n",
    ")\n",
    "\n",
    "# es = Elasticsearch(\n",
    "#     \"https://my-elasticsearch-project-fc9fd1.es.ap-southeast-1.aws.elastic.cloud:443\",\n",
    "#     api_key=\"aXY4NkpKUUJaNVNfUEdnZHdVZ186MExmVk1iclZRWWlrS1hpeDRhOWRGUQ==\"\n",
    "# )\n",
    "\n",
    "from model_config import load_embedding_model_VN2, load_tokenizer2\n",
    "embeddings = load_embedding_model_VN2()\n",
    "tokenizer = load_tokenizer2()\n",
    "json_file_path = r\"D:\\DATN\\QA_System\\casestudy\\pdf\\data0.json\"\n",
    "index_name = \"base_test\"\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"metadata\": {\"type\": \"object\"},\n",
    "            \"vector\": {\"type\": \"dense_vector\", \"dims\": 768, \"similarity\": \"cosine\", \"index\": True}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "if es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "es.indices.create(index=index_name, body=mapping)\n",
    "\n",
    "def generate_actions(documents):\n",
    "    truncated = 0\n",
    "    for i, record in enumerate(documents):\n",
    "        try:\n",
    "            # Get tokens\n",
    "            tokens = tokenizer.encode(record[\"text\"], add_special_tokens=False)\n",
    "            \n",
    "            # Truncate if needed\n",
    "            if len(tokens) > 254:\n",
    "                token = tokens[:236]  # Chỉ lấy 254 token đầu tiên\n",
    "                record[\"text\"] = tokenizer.decode(token)  # Chuyển lại thành text\n",
    "                truncated += 1\n",
    "                print(f\"Truncated document {i} from {len(tokens)} tokens to 236 tokens\")\n",
    "                \n",
    "            embedding = embeddings.embed_query(record[\"text\"])\n",
    "            print(f\"Processing document {i}\")\n",
    "            yield {\n",
    "                \"_op_type\": \"index\",\n",
    "                \"_index\": index_name,\n",
    "                \"_source\": {\n",
    "                    \"text\": record[\"text\"],\n",
    "                    \"metadata\": record[\"metadata\"],\n",
    "                    \"vector\": embedding\n",
    "                }\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing document {i}: {str(e)}\")\n",
    "            raise e\n",
    "    print(f\"Total documents truncated: {truncated}\")\n",
    "\n",
    "\n",
    "with open(json_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "start_time = time()\n",
    "helpers.bulk(es, generate_actions(json_data))\n",
    "print(f\"Indexing completed. Time taken: {time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
