import streamlit as st
import logging
import time
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from dotenv import load_dotenv
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from retrieval import hybrid_retriever , qaretrieval
# Configure logging with UTF-8 encoding
logging.basicConfig(
    level=logging.INFO,
    #format='%(asctime)s - %(levelname)s - %(message)s',
    format = '%(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('app.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)
from model_config import load_gpt4o_mini_model , load_gemini15
from prompt import QA_TEMPLATE, REWRITE_TEMPLATE
COLLECTION_NAME = "raptor"
BASE_COLLECTION_NAME = "base"
QA_COLLECTION_NAME = "faq_data"
THRESHOLD = 0.85
# Kh·ªüi t·∫°o c√°c th√†nh ph·∫ßn global
load_dotenv()
st.set_page_config(page_title="University Assistant", page_icon="ü§ñ")
st.title("University Assistant")
@st.cache_resource
def initialize_model_and_db():
    logger.info("Kh·ªüi t·∫°o model, database v√† FAQ...")
    llm = load_gemini15()
    db = hybrid_retriever(COLLECTION_NAME)
    qa_db = qaretrieval(QA_COLLECTION_NAME)
    return llm, db, qa_db

llm, db ,qa_db= initialize_model_and_db()



def rewrite_question(user_query, formatted_history):
    logger.info("=== B·∫Øt ƒë·∫ßu vi·∫øt l·∫°i c√¢u h·ªèi ===")
    #logger.info(f"C√¢u h·ªèi g·ªëc: {user_query}")
    
    if formatted_history:
        logger.info("ƒêang s·ª≠ d·ª•ng l·ªãch s·ª≠ chat ƒë·ªÉ vi·∫øt l·∫°i c√¢u h·ªèi")
        #logger.info(f"L·ªãch s·ª≠ chat ƒë√£ format: {formatted_history}")
    else:
        logger.info("Kh√¥ng c√≥ l·ªãch s·ª≠ chat tr∆∞·ªõc ƒë√≥")
    
    rewrite_prompt = ChatPromptTemplate.from_template(REWRITE_TEMPLATE)
    chain = rewrite_prompt | llm | StrOutputParser()
    
    logger.info("ƒêang g·ª≠i y√™u c·∫ßu vi·∫øt l·∫°i c√¢u h·ªèi t·ªõi model...")
    rewritten_query = chain.invoke({
        "chat_history": formatted_history,
        "question": user_query
    })
    logger.info(f"c√¢u h·ªèi g·ªëc: {user_query}")
    logger.info(f"C√¢u h·ªèi ƒë√£ ƒë∆∞·ª£c vi·∫øt l·∫°i: {rewritten_query.strip()}")
    logger.info("=== K·∫øt th√∫c vi·∫øt l·∫°i c√¢u h·ªèi ===")
    return rewritten_query.strip()

def format_chat_history(chat_history, max_pairs=5):
    formatted_history = []
    qa_pairs = []

    for i in range(len(chat_history)-1):
        if isinstance(chat_history[i], HumanMessage) and isinstance(chat_history[i+1], AIMessage):
            qa_pairs.append({
                "question": chat_history[i].content,
                "answer": chat_history[i+1].content
            })

    recent_pairs = qa_pairs[-max_pairs:] if qa_pairs else []
    
    for pair in recent_pairs:
        formatted_history.append(f"Human: {pair['question']}\nAssistant: {pair['answer']}")

    return "\n\n".join(formatted_history)
def check_faq(user_query, qa_db = qa_db):
    logger.info("Truy v·∫•n DB cho FAQ...")
    # G·ªçi `semantic_qa_search` ƒë·ªÉ t√¨m ki·∫øm trong collection
    result = qa_db.invoke(user_query)

    if len(result) > 0:
        logger.info("C√¢u h·ªèi ph√π h·ª£p t√¨m th·∫•y trong QADB.")
        return result[0].page_content  # Tr·∫£ v·ªÅ c√¢u tr·∫£ l·ªùi t·ªët nh·∫•t

    logger.info("Kh√¥ng t√¨m th·∫•y c√¢u tr·∫£ l·ªùi ph√π h·ª£p trong QADB.")
    return None
def stream_text(text):
    """Generator function ƒë·ªÉ stream t·ª´ng t·ª´ c·ªßa vƒÉn b·∫£n"""
    time.sleep(1)
    for word in text.split():
        yield word + " "
        # N·∫øu mu·ªën ƒëi·ªÅu ch·ªânh t·ªëc ƒë·ªô stream, c√≥ th·ªÉ th√™m time.sleep()
        time.sleep(0.05)

def get_response(user_query, chat_history,db = db ):
    logger.info("=== B·∫Øt ƒë·∫ßu qu√° tr√¨nh t·∫°o c√¢u tr·∫£ l·ªùi ===")

    # Ki·ªÉm tra trong FAQ
    answer = check_faq(user_query)
    if answer:
        return stream_text(answer)
    #Kh√¥ng t√¨m th·∫•y, g·ªçi ƒë·∫øn RAG
    logger.info("Kh√¥ng t√¨m th·∫•y c√¢u h·ªèi ph√π h·ª£p trong FAQ, chuy·ªÉn ƒë·∫øn h·ªá th·ªëng RAG.")
    # Vi·∫øt l·∫°i c√¢u h·ªèi
        #Format d·ªØ li·ªáu
    logger.info("=== B·∫Øt ƒë·∫ßu format chat history ===")
    formatted_history = format_chat_history(chat_history, max_pairs=5)
    rewritten_query = rewrite_question(user_query, formatted_history)
    if rewritten_query.startswith("<spam>"):
        default_response = "Xin l·ªói, t√¥i ch·ªâ l√† tr·ª£ l√Ω ·∫£o c·ªßa ƒê·∫°i h·ªçc v√† t√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y. N·∫øu b·∫°n c√≥ c√¢u h·ªèi n√†o kh√°c h√£y cho t√¥i bi·∫øt nh√©!"
        logger.info("Ph√°t hi·ªán c√¢u h·ªèi kh√¥ng li√™n quan, tr·∫£ l·ªùi m·∫∑c ƒë·ªãnh.")
        return stream_text(default_response)
    # Truy v·∫•n th√¥ng tin li√™n quan  
    logger.info("=== B·∫Øt ƒë·∫ßu truy xu·∫•t t√†i li·ªáu ===")
    relevant_docs = db.invoke(rewritten_query)
    logger.info(f"S·ªë l∆∞·ª£ng t√†i li·ªáu t√¨m ƒë∆∞·ª£c: {len(relevant_docs)}")

    logger.info("=== K·∫øt th√∫c truy xu·∫•t t√†i li·ªáu ===")


    doc_context = "\n\n".join([doc.page_content for doc in relevant_docs])
    logger.info(f"ƒê·ªô d√†i context: {len(doc_context)} k√Ω t·ª±")
    logger.info("=== K·∫øt th√∫c format d·ªØ li·ªáu ===")

    QAprompt = ChatPromptTemplate.from_template(QA_TEMPLATE)
    logger.info("=== B·∫Øt ƒë·∫ßu t·∫°o c√¢u tr·∫£ l·ªùi ===")
    logger.info(f"=== full QA_prompt {QAprompt}===")
    
    chain = QAprompt | llm | StrOutputParser()
    logger.info("ƒêang g·ª≠i y√™u c·∫ßu t·ªõi model...")
    response = chain.stream({
        "chat_history": formatted_history,
        "documents": doc_context,
        "question": rewritten_query
    })
    logger.info("=== K·∫øt th√∫c t·∫°o c√¢u tr·∫£ l·ªùi ===")
    logger.info(f"C√¢u tr·∫£ l·ªùi: {response}")
    return response
####code m·ªõi #######
def process_user_input(user_query):
    if not user_query:
        return None
    if len(user_query) > 512:
        st.error("ƒê·ªô d√†i c√¢u h·ªèi qu√° d√†i, vui l√≤ng nh·∫≠p c√¢u h·ªèi ng·∫Øn h∆°n!")
        return None
    logger.info(f"\n=== B·∫Øt ƒë·∫ßu x·ª≠ l√Ω c√¢u h·ªèi m·ªõi:{user_query} ===\n")
    st.session_state.chat_history.append(HumanMessage(content=user_query))
    
    with st.chat_message("Human"):
        st.markdown(user_query)
    
    with st.chat_message("AI"):
        response = st.write_stream(get_response(user_query, st.session_state.chat_history))
        logger.info(f"C√¢u tr·∫£ l·ªùi cho ng∆∞·ªùi d√πng: {response}===\n")

    st.session_state.chat_history.append(AIMessage(content=response))
    return response

def display_chat_history():
    if "chat_history" not in st.session_state:
        logger.info("=== Kh·ªüi t·∫°o phi√™n chat m·ªõi ===\n")
        st.session_state.chat_history = []
        st.session_state.chat_history.append(AIMessage(
            content="Ch√†o b·∫°n, T√¥i l√† tr·ª£ l√Ω ·∫£o c·ªßa ƒê·∫°i h·ªçc . T√¥i c√≥ th·ªÉ gi√∫p g√¨ cho b·∫°n?"
        ))
    
    for message in st.session_state.chat_history:
        if isinstance(message, AIMessage):
            with st.chat_message("AI"):
                st.write(message.content)
        elif isinstance(message, HumanMessage):
            with st.chat_message("Human"):
                st.write(message.content)

# Main interaction
display_chat_history()
user_query = st.chat_input("Nh·∫≠p tin nh·∫Øn c·ªßa b·∫°n...")
if user_query:
    process_user_input(user_query)